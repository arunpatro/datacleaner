{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "DELETE_FILES = [\".DS_Store\"]\n",
    "IGNORE_FOLDERS = [\".git\"]\n",
    "\n",
    "class DuplicateGroup(BaseModel):\n",
    "    file_md5: str \n",
    "    file_size_h: str\n",
    "    file_count: int\n",
    "    common_ancestor: str\n",
    "    folder_file: list[tuple[str, str]]\n",
    "\n",
    "\n",
    "class ResolutionCategory(Enum):\n",
    "    same_folder_diff_name = \"same_folder_diff_name\"\n",
    "    diff_folder_same_name = \"diff_folder_same_name\"\n",
    "    \n",
    "    \n",
    "class Resolution(BaseModel):\n",
    "    category: ResolutionCategory\n",
    "    files: list[tuple[str, bool]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_size(size_bytes):\n",
    "    \"\"\"Convert bytes to a readable format (KB, MB, GB, etc.).\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return f\"{s} {size_name[i]}\"\n",
    "\n",
    "def get_duplicate_volume(df):\n",
    "    return convert_size(df[df.duplicated(subset='file_md5', keep='first')].file_size.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/12w744p179x731mgr1zcql2w0000gn/T/ipykernel_24170/4261886176.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dupl_records = duplicates.groupby(['file_md5', 'file_size_h'], group_keys=False).apply(lambda x: x[['file_path', 'folder']].to_dict(orient='records')).to_dict()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../files_report.csv\")\n",
    "df = df[~df.isna().any(axis=1) & (df.file_size > 0)]\n",
    "\n",
    "\n",
    "duplicates = df[df.duplicated(subset='file_md5', keep=False)].sort_values(by='file_size', ascending=False)\n",
    "duplicates['file_size_h'] = duplicates['file_size'].apply(convert_size)\n",
    "duplicates['folder'] = duplicates['file_path'].apply(lambda x: os.path.dirname(x))\n",
    "dupl_records = duplicates.groupby(['file_md5', 'file_size_h'], group_keys=False).apply(lambda x: x[['file_path', 'folder']].to_dict(orient='records')).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(duplicates.file_path.values)\n",
    "folder_paths = [os.path.dirname(p) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphviz import Digraph\n",
    "\n",
    "# class TrieNode:\n",
    "#     def __init__(self, key=\"\"):\n",
    "#         self.key = key\n",
    "#         self.children = {}\n",
    "#         self.is_end_of_path = False\n",
    "\n",
    "# class Trie:\n",
    "#     def __init__(self):\n",
    "#         self.root = TrieNode()\n",
    "\n",
    "#     def insert(self, path):\n",
    "#         current = self.root\n",
    "#         for part in path.split('/'):\n",
    "#             if part not in current.children:\n",
    "#                 current.children[part] = TrieNode(part)\n",
    "#             current = current.children[part]\n",
    "#         current.is_end_of_path = True\n",
    "\n",
    "#     def compress(self):\n",
    "#         def compress_node(node):\n",
    "#             keys_to_compress = list(node.children.keys())\n",
    "#             for key in keys_to_compress:\n",
    "#                 child = node.children[key]\n",
    "#                 while len(child.children) == 1 and not child.is_end_of_path:\n",
    "#                     grandchild_key = next(iter(child.children))\n",
    "#                     grandchild = child.children[grandchild_key]\n",
    "#                     child.key += '/' + grandchild_key\n",
    "#                     child.children = grandchild.children\n",
    "#                     child.is_end_of_path = grandchild.is_end_of_path\n",
    "#                 compress_node(child)\n",
    "        \n",
    "#         compress_node(self.root)\n",
    "\n",
    "#     def display(self):\n",
    "#         dot = Digraph()\n",
    "#         dot.node('root', 'root', style='filled', fillcolor='lightgray')\n",
    "        \n",
    "#         def add_edges(node, parent_key):\n",
    "#             for key, child in node.children.items():\n",
    "#                 if '.' in key:  # Assuming base file names contain a dot (e.g., 'xyz.png')\n",
    "#                     dot.node(child.key, child.key, style='filled', fillcolor='lightgray')\n",
    "#                 else:\n",
    "#                     dot.node(child.key, child.key)\n",
    "#                 dot.edge(parent_key, child.key)\n",
    "#                 add_edges(child, child.key)\n",
    "        \n",
    "#         add_edges(self.root, 'root')\n",
    "#         return dot\n",
    "\n",
    "\n",
    "# # paths = []\n",
    "\n",
    "\n",
    "# trie = Trie()\n",
    "# for path in paths:\n",
    "#     trie.insert(path)\n",
    "\n",
    "# print(\"Before compression:\")\n",
    "# dot = trie.display()\n",
    "# dot.render('trie_before_compression', format='svg', cleanup=True)\n",
    "\n",
    "# trie.compress()\n",
    "\n",
    "# print(\"\\nAfter compression:\")\n",
    "# dot = trie.display()\n",
    "# dot.render('trie_after_compression', format='svg', cleanup=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_common_prefix(paths):\n",
    "    \"\"\"Find the largest common prefix (base folder) in a list of paths.\"\"\"\n",
    "    if not paths:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the paths into lists of directories\n",
    "    split_paths = [p.split(os.sep) for p in paths]\n",
    "    \n",
    "    # Use zip to transpose the list of lists and iterate over corresponding parts\n",
    "    common_parts = []\n",
    "    for parts in zip(*split_paths):\n",
    "        if all(part == parts[0] for part in parts):\n",
    "            common_parts.append(parts[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Join the common parts back into a single path\n",
    "    return os.sep.join(common_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the duplicate groups\n",
    "_dups = []\n",
    "for ix, ((md5, size_h), records) in enumerate(dupl_records.items()):\n",
    "    paths = [r['file_path'] for r in records]\n",
    "\n",
    "    common_ancestor = find_common_prefix(paths)\n",
    "    folder_and_file = [(r['folder'][len(common_ancestor):].lstrip(os.sep), os.path.basename(r['file_path']) )  for r in records]\n",
    "    \n",
    "    # how often these folders appear together\n",
    "    _dups.append(DuplicateGroup(file_md5=md5, file_size_h=size_h, file_count=len(paths), common_ancestor=common_ancestor, folder_file=folder_and_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DuplicateGroup(file_md5='00a989d9a58de3a7e294631061418125', file_size_h='1.43 MB', file_count=2, common_ancestor='/Users/arunpatro/My Drive/IIT - KGP/Acads/Semester 5/Power Electronics Lab', folder_file=[('NED mohan_PE/lab record', 'DSC02861.JPG'), ('Lab Record', 'DSC02861.JPG')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deletion_list(dups: list[DuplicateGroup]) -> list[str]:\n",
    "    \"\"\"Generate a list of deletions for each duplicate group.\"\"\"\n",
    "    x = [dup.files for dup in _dd]\n",
    "    x = list(itertools.chain(*x))\n",
    "    df = pd.DataFrame(x, columns=['file_path', 'keep'])\n",
    "    df['delete'] = ~df['keep']\n",
    "    return df[df['delete']]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common</th>\n",
       "      <th>folder1</th>\n",
       "      <th>folder2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/Acads/Seme...</td>\n",
       "      <td>Lab Record</td>\n",
       "      <td>NED mohan_PE/lab record</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/Acads/Seme...</td>\n",
       "      <td>Power EC/Power EC NPTEL</td>\n",
       "      <td>Power Electronics Lab/Power EC NPTEL</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/arunpatro/My Drive</td>\n",
       "      <td>Google Photos/2017</td>\n",
       "      <td>IIT - KGP/Acads/Instru Lab Expts/expt_6_photos</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/Acads/BTP/...</td>\n",
       "      <td>backs</td>\n",
       "      <td>shankar-2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/arunpatro/My Drive</td>\n",
       "      <td>Google Photos/2015</td>\n",
       "      <td>IIT - KGP/Acads/Semester 5/Quantum/3D + Quantum</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/Acads/BTP/...</td>\n",
       "      <td></td>\n",
       "      <td>refs/heads</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>/Users/arunpatro/My Drive</td>\n",
       "      <td>ARCHIVE/SOP Samples from Raunak</td>\n",
       "      <td>Graduate School Applications</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>/Users/arunpatro/My Drive</td>\n",
       "      <td>Books/Machine Learning</td>\n",
       "      <td>Myntra/pdfs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/Acads/MTP/...</td>\n",
       "      <td>figures</td>\n",
       "      <td>imgs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>/Users/arunpatro/My Drive/IIT - KGP/IIT Requir...</td>\n",
       "      <td></td>\n",
       "      <td>IIT Medicine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                common  \\\n",
       "0    /Users/arunpatro/My Drive/IIT - KGP/Acads/Seme...   \n",
       "1    /Users/arunpatro/My Drive/IIT - KGP/Acads/Seme...   \n",
       "2                            /Users/arunpatro/My Drive   \n",
       "3    /Users/arunpatro/My Drive/IIT - KGP/Acads/BTP/...   \n",
       "4                            /Users/arunpatro/My Drive   \n",
       "..                                                 ...   \n",
       "107  /Users/arunpatro/My Drive/IIT - KGP/Acads/BTP/...   \n",
       "108                          /Users/arunpatro/My Drive   \n",
       "109                          /Users/arunpatro/My Drive   \n",
       "110  /Users/arunpatro/My Drive/IIT - KGP/Acads/MTP/...   \n",
       "111  /Users/arunpatro/My Drive/IIT - KGP/IIT Requir...   \n",
       "\n",
       "                             folder1  \\\n",
       "0                         Lab Record   \n",
       "1            Power EC/Power EC NPTEL   \n",
       "2                 Google Photos/2017   \n",
       "3                              backs   \n",
       "4                 Google Photos/2015   \n",
       "..                               ...   \n",
       "107                                    \n",
       "108  ARCHIVE/SOP Samples from Raunak   \n",
       "109           Books/Machine Learning   \n",
       "110                          figures   \n",
       "111                                    \n",
       "\n",
       "                                             folder2  count  \n",
       "0                            NED mohan_PE/lab record     46  \n",
       "1               Power Electronics Lab/Power EC NPTEL     44  \n",
       "2     IIT - KGP/Acads/Instru Lab Expts/expt_6_photos     39  \n",
       "3                                          shankar-2     38  \n",
       "4    IIT - KGP/Acads/Semester 5/Quantum/3D + Quantum     22  \n",
       "..                                               ...    ...  \n",
       "107                                       refs/heads      1  \n",
       "108                     Graduate School Applications      1  \n",
       "109                                      Myntra/pdfs      1  \n",
       "110                                             imgs      1  \n",
       "111                                     IIT Medicine      1  \n",
       "\n",
       "[112 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Initialize a Counter to store pairwise occurrences\n",
    "pairwise_counts = Counter()\n",
    "\n",
    "dup_folders = [(d.common_ancestor, d.folder_file) for d in _dups if any(f != \"\" for f, _ in d.folder_file)]\n",
    "dup_folders = [(x, *[i[0] for i in y]) for x, y in dup_folders]\n",
    "\n",
    "for acs, *folders in dup_folders:\n",
    "    sorted_folders = sorted(folders)\n",
    "    unique_pairs = [(acs, *sorted(pair)) for pair in itertools.combinations(sorted_folders, 2) if pair[0] != pair[1]]\n",
    "    pairwise_counts.update(unique_pairs)\n",
    "\n",
    "# Printing the pairwise occurrences\n",
    "vals = []\n",
    "for pair, count in pairwise_counts.most_common():\n",
    "    vals.append((*pair, count))\n",
    "    \n",
    "pd.DataFrame(vals, columns=['common', 'folder1', 'folder2', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common: /Users/arunpatro/My Drive/IIT - KGP/Acads/Semester 5/Power Electronics Lab\n",
      "Clashes: 46\n",
      "Choosing \"NED mohan_PE/lab record\" over \"Lab Record\"\n",
      "--\n",
      "F1: Lab Record\n",
      "\tClashes: (31.5%) - 46 / 146\n",
      "\n",
      "F2: NED mohan_PE/lab record\n",
      "\tClashes: (37.1%) - 46 / 124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate MD5 hash of a file\n",
    "def calculate_md5(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# Function to recursively find all files and their MD5s in a directory\n",
    "def find_all_files_with_md5(directory):\n",
    "    file_md5s = set()\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_md5 = calculate_md5(file_path)\n",
    "            if file_md5:\n",
    "                file_md5s.add(file_md5)\n",
    "    return file_md5s\n",
    "\n",
    "# Function to ask user for their preference\n",
    "def get_user_preference(pair, count):\n",
    "    base, folder1, folder2 = pair\n",
    "    files1 = find_all_files_with_md5(os.path.join(base, folder1))\n",
    "    files2 = find_all_files_with_md5(os.path.join(base, folder2))\n",
    "    f1_total = len(files1)\n",
    "    f2_total = len(files2)\n",
    "    common_files = files1 & files2\n",
    "    # assert len(common_files) == count\n",
    "    # only_in_f1 = files1 - files2\n",
    "    # only_in_f2 = files2 - files1\n",
    "    \n",
    "    clash_perc1 = len(common_files) / f1_total if f1_total > 0 else 0\n",
    "    clash_perc2 = len(common_files) / f2_total if f2_total > 0 else 0\n",
    "    \n",
    "    choices = [(folder1, clash_perc1), (folder2, clash_perc2)]\n",
    "    choices = sorted(choices, key=lambda x: x[1], reverse=True)\n",
    "    chosen_folder = choices[0][0]\n",
    "    \n",
    "    output = f\"\"\"\n",
    "Common: {base}\n",
    "Clashes: {count}\n",
    "Choosing \"{choices[0][0]}\" over \"{choices[1][0]}\"\n",
    "--\n",
    "F1: {folder1}\n",
    "\\tClashes: ({clash_perc1:.1%}) - {len(common_files)} / {f1_total}\n",
    "\n",
    "F2: {folder2}\n",
    "\\tClashes: ({clash_perc2:.1%}) - {len(common_files)} / {f2_total}\n",
    "\"\"\"\n",
    "    print(output)\n",
    "    \n",
    "    return chosen_folder\n",
    "\n",
    "# Iterate over the pairs and get user preferences\n",
    "user_preferences = {}\n",
    "for pair, count in pairwise_counts.items():\n",
    "    preferred_folder = get_user_preference(pair, count)\n",
    "    if preferred_folder:\n",
    "        user_preferences[pair] = preferred_folder\n",
    "\n",
    "# Save user preferences to a JSON file\n",
    "with open('user_preferences.json', 'w') as f:\n",
    "    json.dump(user_preferences, f, indent=4)\n",
    "\n",
    "print(\"User preferences saved to user_preferences.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_same_folder(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates in the same folder.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    file_names = [f for _, f in folder_files]\n",
    "    files = sorted(file_names, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    files = [os.path.join(dupl_group.common_ancestor, f) for f in files]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return Resolution(files=files, category=ResolutionCategory.same_folder_diff_name)\n",
    "\n",
    "def resolve_same_name(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates with the same name.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    base_file = folder_files[0][1]\n",
    "    folders = [f for f, _ in folder_files]\n",
    "    folders = sorted(folders, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    folders = [os.path.join(dupl_group.common_ancestor, f) for f in folders]\n",
    "    files = [os.path.join(f, base_file) for f in folders]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return None\n",
    "    return Resolution(files=files, category=ResolutionCategory.diff_folder_same_name)\n",
    "\n",
    "def resolve(group: DuplicateGroup) -> Resolution:\n",
    "    if all([f == \"\" for f, _ in group.folder_file]):\n",
    "        # all files are in the same folder\n",
    "        return resolve_same_folder(group)\n",
    "    elif len(set([f for _, f in group.folder_file])) == 1:\n",
    "        # all files have the same name\n",
    "        return resolve_same_name(group)\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dd = [resolve(d) for d in _dups if resolve(d) is not None]\n",
    "xx = pd.DataFrame([dup.files for dup in _dd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat()\n",
    "df.columns = ['file_path', 'keep']\n",
    "df['delete'] = ~df['keep']\n",
    "return df[df['delete']]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dupl2 = [d for d in _dups if not resolve(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dupl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_folder_dupls = [pd.DataFrame(resolve(d).files) for d in _dups if resolve(d) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(same_folder_dupls)\n",
    "df.columns = ['file_path', 'keep']\n",
    "for x in (df[~df['keep']].file_path.values):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_folders(folder_paths):\n",
    "    \"\"\"Return the arg idxs - Prioritize Shorter and more discriptive folder first.\"\"\"\n",
    "    return sorted(range(len(folder_paths)), key=lambda x: (len(folder_paths[x]), folder_paths[x]))\n",
    "\n",
    "def duplicates_in_same_folder(records):\n",
    "    \"\"\"Find duplicates in the same folder.\"\"\"\n",
    "\n",
    "    if len(records) > 1:\n",
    "        common_prefix = find_common_prefix([r['folder'] for r in records])\n",
    "        folder_remainders = [r['folder'][len(common_prefix):].lstrip(os.sep) for r in records]\n",
    "        file_names = [os.path.basename(r['file_path']) for r in records]\n",
    "        if common_prefix and all([f == \"\" for f in folder_remainders]):\n",
    "            # sorted by max length first and then reverse alphabetically\n",
    "            \n",
    "\n",
    "            df = pd.DataFrame(items)\n",
    "            df.columns = ['File Name']\n",
    "            df['Keep'] = False\n",
    "            df.loc[df.index[-1], 'Keep'] = True\n",
    "            return df\n",
    "        \n",
    "    return pd.DataFrame()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "class Resolution(BaseModel):\n",
    "    \"\"\"Resolution for duplicates.\"\"\"\n",
    "    resolution: pd.DataFrame\n",
    "    category: Literal['same_folder', 'different_folders', 'None']\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return not self.resolution.empty\n",
    "    \n",
    "    \n",
    "def resolve_duplicates(records) -> Resolution:\n",
    "    \n",
    "    if len(records) > 1:\n",
    "        \"\"\"Resolve duplicates in the same folder.\"\"\"\n",
    "        common_prefix = find_common_prefix([r['folder'] for r in records])\n",
    "        folder_remainders = [r['folder'][len(common_prefix):].lstrip(os.sep) for r in records]\n",
    "        file_names = [os.path.basename(r['file_path']) for r in records]\n",
    "        if common_prefix and all([f == \"\" for f in folder_remainders]):\n",
    "            # sorted by max length first and then reverse alphabetically\n",
    "            items = sorted(file_names, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "\n",
    "            df = pd.DataFrame(items)\n",
    "            df.columns = ['File Name']\n",
    "            df['Keep'] = False\n",
    "            df.loc[df.index[-1], 'Keep'] = True\n",
    "            return Resolution(resolution=df, category='same_folder')\n",
    "    \n",
    "    \n",
    "        \n",
    "    return Resolution(resolution=pd.DataFrame(), category='None')\n",
    "        \n",
    "def duplicates_in_different_folders(records):\n",
    "    \n",
    "    if len(records) > 2:\n",
    "        ## files could be spread across multiple folders and in each folder there could be multiple duplicates\n",
    "        ## group by folder and then remove duplicates in each folder\n",
    "        df = pd.DataFrame(records)\n",
    "        df = df.groupby('folder').apply(resolve_duplicates)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return pd.DataFrame()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the groups and print the paths with highlighted common base folder\n",
    "for ix, ((md5, size_h), records) in enumerate(dupl_records.items()):\n",
    "    md5_short = md5[:6]\n",
    "\n",
    "    \n",
    "    # _resolution = resolve_duplicates(records)\n",
    "    \n",
    "    # Print MD5 hash, size, and common prefix\n",
    "    print(f\"{ix+1}.\")\n",
    "    print(f\"MD5: {md5_short} Size: {size_h}\")\n",
    "    print(f\"Common: {nearest_ancestor}\")\n",
    "\n",
    "    # _action_df = duplicates_in_same_folder(v)\n",
    "    # if not _action_df.empty:\n",
    "    #     print(f\"\"\"--\n",
    "    # Reason: Same folder duplicate\n",
    "    # Details:\n",
    "    # - Common Folder: {common_prefix}\n",
    "    # - N Duplicate Files: {len(_action_df)-1}\n",
    "    # {_action_df}\n",
    "    # --\"\"\")\n",
    "    for i, (folder, filepath) in enumerate(folder_and_file):\n",
    "        # Print the details\n",
    "        print(f\"  {i+1}. Folder: {folder}\")\n",
    "        print(f\"     File: {filepath}\")\n",
    "    print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = duplicates.groupby('file_md5').agg({'folder': list}).reset_index()\n",
    "\n",
    "# grouped_duplicates = duplicates.sort_values(by='file_size', ascending=False).groupby('file_md5')\n",
    "\n",
    "# # Aggregate base folders into a list\n",
    "# aggregated_duplicates = grouped_duplicates.agg({\n",
    "#     'file_path': list,\n",
    "#     'file_size': 'sum',\n",
    "#     'base_folder': lambda x: list(set(x))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.to_csv(\"duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_duplicates = duplicates.sort_values(by='file_size', ascending=False).groupby('file_md5')\n",
    "\n",
    "# Iterate over groups and display file paths and sizes\n",
    "for md5, group in grouped_duplicates:\n",
    "    print(f\"\\nMD5 Hash: {md5}\")\n",
    "    print(f\"Total Size: {convert_size(group['file_size'].sum())}\")\n",
    "    print(\"File Paths:\")\n",
    "    for idx, row in group.iterrows():\n",
    "        print(f\"  {row['file_path']} - {row['file_size_h']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
