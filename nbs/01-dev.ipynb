{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "DELETE_FILES = [\".DS_Store\"]\n",
    "IGNORE_FOLDERS = [\".git\"]\n",
    "\n",
    "class DuplicateGroup(BaseModel):\n",
    "    file_md5: str \n",
    "    file_size_h: str\n",
    "    file_count: int\n",
    "    common_ancestor: str\n",
    "    folder_file: list[tuple[str, str]]\n",
    "\n",
    "\n",
    "class ResolutionCategory(Enum):\n",
    "    same_folder_diff_name = \"same_folder_diff_name\"\n",
    "    diff_folder_same_name = \"diff_folder_same_name\"\n",
    "    \n",
    "    \n",
    "class Resolution(BaseModel):\n",
    "    category: ResolutionCategory\n",
    "    files: list[tuple[str, bool]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_size(size_bytes):\n",
    "    \"\"\"Convert bytes to a readable format (KB, MB, GB, etc.).\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return f\"{s} {size_name[i]}\"\n",
    "\n",
    "def find_common_prefix(paths):\n",
    "    \"\"\"Find the largest common prefix (base folder) in a list of paths.\"\"\"\n",
    "    if not paths:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the paths into lists of directories\n",
    "    split_paths = [p.split(os.sep) for p in paths]\n",
    "    \n",
    "    # Use zip to transpose the list of lists and iterate over corresponding parts\n",
    "    common_parts = []\n",
    "    for parts in zip(*split_paths):\n",
    "        if all(part == parts[0] for part in parts):\n",
    "            common_parts.append(parts[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Join the common parts back into a single path\n",
    "    return os.sep.join(common_parts)\n",
    "\n",
    "def get_duplicate_volume(df):\n",
    "    return convert_size(df[df.duplicated(subset='file_md5', keep='first')].file_size.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../files_report.csv\")\n",
    "df = df[~df.isna().any(axis=1) & (df.file_size > 0)]\n",
    "\n",
    "duplicates = df[df.duplicated(subset='file_md5', keep=False)].sort_values(by='file_size', ascending=False)\n",
    "duplicates['file_size_h'] = duplicates['file_size'].apply(convert_size)\n",
    "duplicates['folder'] = duplicates['file_path'].apply(lambda x: os.path.dirname(x))\n",
    "dupl_records = duplicates.groupby(['file_md5', 'file_size_h'], group_keys=False).apply(lambda x: x[['file_path', 'folder']].to_dict(orient='records')).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(duplicates.file_path.values)\n",
    "folder_paths = [os.path.dirname(p) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the duplicate groups\n",
    "_dups = []\n",
    "for ix, ((md5, size_h), records) in enumerate(dupl_records.items()):\n",
    "    paths = [r['file_path'] for r in records]\n",
    "\n",
    "    common_ancestor = find_common_prefix(paths)\n",
    "    folder_and_file = [(r['folder'][len(common_ancestor):].lstrip(os.sep), os.path.basename(r['file_path']) )  for r in records]\n",
    "    \n",
    "    # how often these folders appear together\n",
    "    _dups.append(DuplicateGroup(file_md5=md5, file_size_h=size_h, file_count=len(paths), common_ancestor=common_ancestor, folder_file=folder_and_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deletion_list(dups: list[DuplicateGroup]) -> list[str]:\n",
    "    \"\"\"Generate a list of deletions for each duplicate group.\"\"\"\n",
    "    import itertools\n",
    "    x = [dup.files for dup in dups]\n",
    "    x = list(itertools.chain(*x))\n",
    "    df = pd.DataFrame(x, columns=['file_path', 'keep'])\n",
    "    df['delete'] = ~df['keep']\n",
    "    return df[df['delete']]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Initialize a Counter to store pairwise occurrences\n",
    "pairwise_counts = Counter()\n",
    "\n",
    "dup_folders = [(d.common_ancestor, d.folder_file) for d in _dups if any(f != \"\" for f, _ in d.folder_file)]\n",
    "dup_folders = [(x, *[i[0] for i in y]) for x, y in dup_folders]\n",
    "\n",
    "for acs, *folders in dup_folders:\n",
    "    sorted_folders = sorted(folders)\n",
    "    unique_pairs = [(acs, *sorted(pair)) for pair in itertools.combinations(sorted_folders, 2) if pair[0] != pair[1]]\n",
    "    pairwise_counts.update(unique_pairs)\n",
    "\n",
    "# Printing the pairwise occurrences\n",
    "vals = []\n",
    "for pair, count in pairwise_counts.most_common():\n",
    "    vals.append((*pair, count))\n",
    "    \n",
    "pairwise_df = pd.DataFrame(vals, columns=['common', 'folder1', 'folder2', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate MD5 hash of a file\n",
    "def calculate_md5(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# Function to recursively find all files and their MD5s in a directory\n",
    "def find_all_files_with_md5(directory):\n",
    "    file_md5s = set()\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_md5 = calculate_md5(file_path)\n",
    "            if file_md5:\n",
    "                file_md5s.add(file_md5)\n",
    "    return file_md5s\n",
    "\n",
    "# Function to ask user for their preference\n",
    "def get_user_preference(pair, count):\n",
    "    base, folder1, folder2 = pair\n",
    "    files1 = find_all_files_with_md5(os.path.join(base, folder1))\n",
    "    files2 = find_all_files_with_md5(os.path.join(base, folder2))\n",
    "    f1_total = len(files1)\n",
    "    f2_total = len(files2)\n",
    "    common_files = files1 & files2\n",
    "    # assert len(common_files) == count\n",
    "    # only_in_f1 = files1 - files2\n",
    "    # only_in_f2 = files2 - files1\n",
    "    \n",
    "    clash_perc1 = len(common_files) / f1_total if f1_total > 0 else 0\n",
    "    clash_perc2 = len(common_files) / f2_total if f2_total > 0 else 0\n",
    "    \n",
    "    choices = [(folder1, clash_perc1), (folder2, clash_perc2)]\n",
    "    choices = sorted(choices, key=lambda x: x[1], reverse=True)\n",
    "    chosen_folder = choices[0][0]\n",
    "    \n",
    "    output = f\"\"\"\n",
    "Common: {base}\n",
    "Clashes: {count}\n",
    "Choosing \"{choices[0][0]}\" over \"{choices[1][0]}\"\n",
    "--\n",
    "F1: {folder1}\n",
    "\\tClashes: ({clash_perc1:.1%}) - {len(common_files)} / {f1_total}\n",
    "\n",
    "F2: {folder2}\n",
    "\\tClashes: ({clash_perc2:.1%}) - {len(common_files)} / {f2_total}\n",
    "\"\"\"\n",
    "    print(output)\n",
    "    \n",
    "    return chosen_folder\n",
    "\n",
    "# Iterate over the pairs and get user preferences\n",
    "user_preferences = {}\n",
    "for pair, count in pairwise_counts.items():\n",
    "    continue\n",
    "    preferred_folder = get_user_preference(pair, count)\n",
    "    if preferred_folder:\n",
    "        user_preferences[pair] = preferred_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_same_folder(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates in the same folder.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    file_names = [f for _, f in folder_files]\n",
    "    files = sorted(file_names, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    files = [os.path.join(dupl_group.common_ancestor, f) for f in files]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return Resolution(files=files, category=ResolutionCategory.same_folder_diff_name)\n",
    "\n",
    "def resolve_same_name(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates with the same name.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    base_file = folder_files[0][1]\n",
    "    folders = [f for f, _ in folder_files]\n",
    "    folders = sorted(folders, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    folders = [os.path.join(dupl_group.common_ancestor, f) for f in folders]\n",
    "    files = [os.path.join(f, base_file) for f in folders]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return Resolution(files=files, category=ResolutionCategory.diff_folder_same_name)\n",
    "\n",
    "def resolve(group: DuplicateGroup) -> Resolution:\n",
    "    if all([f == \"\" for f, _ in group.folder_file]):\n",
    "        # all files are in the same folder\n",
    "        return resolve_same_folder(group)\n",
    "    elif len(set([f for _, f in group.folder_file])) == 1:\n",
    "        # all files have the same name\n",
    "        return resolve_same_name(group)\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_deletes = [resolve(d) for d in _dups if resolve(d) is not None and resolve(d).category == ResolutionCategory.same_folder_diff_name]\n",
    "files_to_delete = generate_deletion_list(easy_deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
