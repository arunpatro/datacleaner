{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "class DuplicateGroup(BaseModel):\n",
    "    file_md5: str \n",
    "    file_size_h: str\n",
    "    file_count: int\n",
    "    common_ancestor: str\n",
    "    folder_file: list[tuple[str, str]]\n",
    "\n",
    "\n",
    "class ResolutionCategory(Enum):\n",
    "    same_folder_diff_name = \"same_folder_diff_name\"\n",
    "    diff_folder_same_name = \"diff_folder_same_name\"\n",
    "    \n",
    "    \n",
    "class Resolution(BaseModel):\n",
    "    category: ResolutionCategory\n",
    "    files: list[tuple[str, bool]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_size(size_bytes):\n",
    "    \"\"\"Convert bytes to a readable format (KB, MB, GB, etc.).\"\"\"\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return f\"{s} {size_name[i]}\"\n",
    "\n",
    "def get_duplicate_volume(df):\n",
    "    return convert_size(df[df.duplicated(subset='file_md5', keep='first')].file_size.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../files_report.csv\")\n",
    "df = df[~df.isna().any(axis=1) & (df.file_size > 0)]\n",
    "\n",
    "\n",
    "duplicates = df[df.duplicated(subset='file_md5', keep=False)].sort_values(by='file_size', ascending=False)\n",
    "duplicates['file_size_h'] = duplicates['file_size'].apply(convert_size)\n",
    "duplicates['folder'] = duplicates['file_path'].apply(lambda x: os.path.dirname(x))\n",
    "# dupl_records = duplicates.groupby(['file_md5', 'file_size_h'], group_keys=False).apply(lambda x: x[['file_path', 'folder']].to_dict(orient='records')).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(duplicates.file_path.values)\n",
    "\n",
    "paths = [os.path.dirname(p) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before compression:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After compression:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trie_after_compression.svg'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self, key=\"\"):\n",
    "        self.key = key\n",
    "        self.children = {}\n",
    "        self.is_end_of_path = False\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, path):\n",
    "        current = self.root\n",
    "        for part in path.split('/'):\n",
    "            if part not in current.children:\n",
    "                current.children[part] = TrieNode(part)\n",
    "            current = current.children[part]\n",
    "        current.is_end_of_path = True\n",
    "\n",
    "    def compress(self):\n",
    "        def compress_node(node):\n",
    "            keys_to_compress = list(node.children.keys())\n",
    "            for key in keys_to_compress:\n",
    "                child = node.children[key]\n",
    "                while len(child.children) == 1 and not child.is_end_of_path:\n",
    "                    grandchild_key = next(iter(child.children))\n",
    "                    grandchild = child.children[grandchild_key]\n",
    "                    child.key += '/' + grandchild_key\n",
    "                    child.children = grandchild.children\n",
    "                    child.is_end_of_path = grandchild.is_end_of_path\n",
    "                compress_node(child)\n",
    "        \n",
    "        compress_node(self.root)\n",
    "\n",
    "    def display(self):\n",
    "        dot = Digraph()\n",
    "        dot.node('root', 'root', style='filled', fillcolor='lightgray')\n",
    "        \n",
    "        def add_edges(node, parent_key):\n",
    "            for key, child in node.children.items():\n",
    "                if '.' in key:  # Assuming base file names contain a dot (e.g., 'xyz.png')\n",
    "                    dot.node(child.key, child.key, style='filled', fillcolor='lightgray')\n",
    "                else:\n",
    "                    dot.node(child.key, child.key)\n",
    "                dot.edge(parent_key, child.key)\n",
    "                add_edges(child, child.key)\n",
    "        \n",
    "        add_edges(self.root, 'root')\n",
    "        return dot\n",
    "\n",
    "\n",
    "# paths = []\n",
    "\n",
    "\n",
    "trie = Trie()\n",
    "for path in paths:\n",
    "    trie.insert(path)\n",
    "\n",
    "print(\"Before compression:\")\n",
    "dot = trie.display()\n",
    "dot.render('trie_before_compression', format='svg', cleanup=True)\n",
    "\n",
    "trie.compress()\n",
    "\n",
    "print(\"\\nAfter compression:\")\n",
    "dot = trie.display()\n",
    "dot.render('trie_after_compression', format='svg', cleanup=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_common_prefix(paths):\n",
    "    \"\"\"Find the largest common prefix (base folder) in a list of paths.\"\"\"\n",
    "    if not paths:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the paths into lists of directories\n",
    "    split_paths = [p.split(os.sep) for p in paths]\n",
    "    \n",
    "    # Use zip to transpose the list of lists and iterate over corresponding parts\n",
    "    common_parts = []\n",
    "    for parts in zip(*split_paths):\n",
    "        if all(part == parts[0] for part in parts):\n",
    "            common_parts.append(parts[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Join the common parts back into a single path\n",
    "    return os.sep.join(common_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the duplicate groups\n",
    "_dups = []\n",
    "for ix, ((md5, size_h), records) in enumerate(dupl_records.items()):\n",
    "    paths = [r['file_path'] for r in records]\n",
    "\n",
    "    common_ancestor = find_common_prefix(paths)\n",
    "    folder_and_file = [(r['folder'][len(common_ancestor):].lstrip(os.sep), os.path.basename(r['file_path']) )  for r in records]\n",
    "    \n",
    "    _dups.append(DuplicateGroup(file_md5=md5, file_size_h=size_h, file_count=len(paths), common_ancestor=common_ancestor, folder_file=folder_and_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deletion_list(dups: list[DuplicateGroup]) -> list[str]:\n",
    "    \"\"\"Generate a list of deletions for each duplicate group.\"\"\"\n",
    "    x = [dup.files for dup in _dd]\n",
    "    x = list(itertools.chain(*x))\n",
    "    df = pd.DataFrame(x, columns=['file_path', 'keep'])\n",
    "    df['delete'] = ~df['keep']\n",
    "    return df[df['delete']]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_folders = [(d.common_ancestor, d.folder_file) for d in _dups if any(f != \"\" for f, _ in d.folder_file)]\n",
    "dup_folders = [(x, *[i[0] for i in y]) for x, y in dup_folders]\n",
    "dup_folders\n",
    "for a, *fs in dup_folders:\n",
    "    print(a)\n",
    "    for f in fs:\n",
    "        print(\"F \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC CLEANUP\n",
    "\n",
    "FILES_TO_DELETE = [\".DS_Store\"]\n",
    "\n",
    "def empty_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Initialize a Counter to store pairwise occurrences\n",
    "pairwise_counts = Counter()\n",
    "\n",
    "# Iterate through dup_folders to count pairwise occurrences\n",
    "for acs, *folders in dup_folders:\n",
    "    # Sort folders to ensure consistent pairs (folder1, folder2) and (folder2, folder1) are counted the same\n",
    "    sorted_folders = sorted(folders)\n",
    "    # Generate all unique pairs of folders and update the Counter\n",
    "    unique_pairs = [(acs, *sorted(pair)) for pair in itertools.combinations(sorted_folders, 2) if pair[0] != pair[1]]\n",
    "    pairwise_counts.update(unique_pairs)\n",
    "\n",
    "\n",
    "# Printing the pairwise occurrences\n",
    "for pair, count in pairwise_counts.most_common():\n",
    "    print(f\"Pair: {pair}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_same_folder(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates in the same folder.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    file_names = [f for _, f in folder_files]\n",
    "    files = sorted(file_names, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    files = [os.path.join(dupl_group.common_ancestor, f) for f in files]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return Resolution(files=files, category=ResolutionCategory.same_folder_diff_name)\n",
    "\n",
    "def resolve_same_name(dupl_group) -> Resolution:\n",
    "    \"\"\"Resolve duplicates with the same name.\"\"\"\n",
    "    # Check if the files are the same\n",
    "    folder_files = dupl_group.folder_file\n",
    "    base_file = folder_files[0][1]\n",
    "    folders = [f for f, _ in folder_files]\n",
    "    folders = sorted(folders, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "    folders = [os.path.join(dupl_group.common_ancestor, f) for f in folders]\n",
    "    files = [os.path.join(f, base_file) for f in folders]\n",
    "    files = [(f, True) if f == files[-1] else (f, False) for f in files] # keep the last file\n",
    "    return None\n",
    "    return Resolution(files=files, category=ResolutionCategory.diff_folder_same_name)\n",
    "\n",
    "def resolve(group: DuplicateGroup) -> Resolution:\n",
    "    if all([f == \"\" for f, _ in group.folder_file]):\n",
    "        # all files are in the same folder\n",
    "        return resolve_same_folder(group)\n",
    "    elif len(set([f for _, f in group.folder_file])) == 1:\n",
    "        # all files have the same name\n",
    "        return resolve_same_name(group)\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dd = [resolve(d) for d in _dups if resolve(d) is not None]\n",
    "xx = pd.DataFrame([dup.files for dup in _dd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat()\n",
    "df.columns = ['file_path', 'keep']\n",
    "df['delete'] = ~df['keep']\n",
    "return df[df['delete']]['file_path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dupl2 = [d for d in _dups if not resolve(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dupl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_folder_dupls = [pd.DataFrame(resolve(d).files) for d in _dups if resolve(d) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(same_folder_dupls)\n",
    "df.columns = ['file_path', 'keep']\n",
    "for x in (df[~df['keep']].file_path.values):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_folders(folder_paths):\n",
    "    \"\"\"Return the arg idxs - Prioritize Shorter and more discriptive folder first.\"\"\"\n",
    "    return sorted(range(len(folder_paths)), key=lambda x: (len(folder_paths[x]), folder_paths[x]))\n",
    "\n",
    "def duplicates_in_same_folder(records):\n",
    "    \"\"\"Find duplicates in the same folder.\"\"\"\n",
    "\n",
    "    if len(records) > 1:\n",
    "        common_prefix = find_common_prefix([r['folder'] for r in records])\n",
    "        folder_remainders = [r['folder'][len(common_prefix):].lstrip(os.sep) for r in records]\n",
    "        file_names = [os.path.basename(r['file_path']) for r in records]\n",
    "        if common_prefix and all([f == \"\" for f in folder_remainders]):\n",
    "            # sorted by max length first and then reverse alphabetically\n",
    "            \n",
    "\n",
    "            df = pd.DataFrame(items)\n",
    "            df.columns = ['File Name']\n",
    "            df['Keep'] = False\n",
    "            df.loc[df.index[-1], 'Keep'] = True\n",
    "            return df\n",
    "        \n",
    "    return pd.DataFrame()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "class Resolution(BaseModel):\n",
    "    \"\"\"Resolution for duplicates.\"\"\"\n",
    "    resolution: pd.DataFrame\n",
    "    category: Literal['same_folder', 'different_folders', 'None']\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return not self.resolution.empty\n",
    "    \n",
    "    \n",
    "def resolve_duplicates(records) -> Resolution:\n",
    "    \n",
    "    if len(records) > 1:\n",
    "        \"\"\"Resolve duplicates in the same folder.\"\"\"\n",
    "        common_prefix = find_common_prefix([r['folder'] for r in records])\n",
    "        folder_remainders = [r['folder'][len(common_prefix):].lstrip(os.sep) for r in records]\n",
    "        file_names = [os.path.basename(r['file_path']) for r in records]\n",
    "        if common_prefix and all([f == \"\" for f in folder_remainders]):\n",
    "            # sorted by max length first and then reverse alphabetically\n",
    "            items = sorted(file_names, key=lambda x: (len(x), x.lower()), reverse=True)\n",
    "\n",
    "            df = pd.DataFrame(items)\n",
    "            df.columns = ['File Name']\n",
    "            df['Keep'] = False\n",
    "            df.loc[df.index[-1], 'Keep'] = True\n",
    "            return Resolution(resolution=df, category='same_folder')\n",
    "    \n",
    "    \n",
    "        \n",
    "    return Resolution(resolution=pd.DataFrame(), category='None')\n",
    "        \n",
    "def duplicates_in_different_folders(records):\n",
    "    \n",
    "    if len(records) > 2:\n",
    "        ## files could be spread across multiple folders and in each folder there could be multiple duplicates\n",
    "        ## group by folder and then remove duplicates in each folder\n",
    "        df = pd.DataFrame(records)\n",
    "        df = df.groupby('folder').apply(resolve_duplicates)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return pd.DataFrame()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the groups and print the paths with highlighted common base folder\n",
    "for ix, ((md5, size_h), records) in enumerate(dupl_records.items()):\n",
    "    md5_short = md5[:6]\n",
    "\n",
    "    \n",
    "    # _resolution = resolve_duplicates(records)\n",
    "    \n",
    "    # Print MD5 hash, size, and common prefix\n",
    "    print(f\"{ix+1}.\")\n",
    "    print(f\"MD5: {md5_short} Size: {size_h}\")\n",
    "    print(f\"Common: {nearest_ancestor}\")\n",
    "\n",
    "    # _action_df = duplicates_in_same_folder(v)\n",
    "    # if not _action_df.empty:\n",
    "    #     print(f\"\"\"--\n",
    "    # Reason: Same folder duplicate\n",
    "    # Details:\n",
    "    # - Common Folder: {common_prefix}\n",
    "    # - N Duplicate Files: {len(_action_df)-1}\n",
    "    # {_action_df}\n",
    "    # --\"\"\")\n",
    "    for i, (folder, filepath) in enumerate(folder_and_file):\n",
    "        # Print the details\n",
    "        print(f\"  {i+1}. Folder: {folder}\")\n",
    "        print(f\"     File: {filepath}\")\n",
    "    print(\"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = duplicates.groupby('file_md5').agg({'folder': list}).reset_index()\n",
    "\n",
    "# grouped_duplicates = duplicates.sort_values(by='file_size', ascending=False).groupby('file_md5')\n",
    "\n",
    "# # Aggregate base folders into a list\n",
    "# aggregated_duplicates = grouped_duplicates.agg({\n",
    "#     'file_path': list,\n",
    "#     'file_size': 'sum',\n",
    "#     'base_folder': lambda x: list(set(x))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.to_csv(\"duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_duplicates = duplicates.sort_values(by='file_size', ascending=False).groupby('file_md5')\n",
    "\n",
    "# Iterate over groups and display file paths and sizes\n",
    "for md5, group in grouped_duplicates:\n",
    "    print(f\"\\nMD5 Hash: {md5}\")\n",
    "    print(f\"Total Size: {convert_size(group['file_size'].sum())}\")\n",
    "    print(\"File Paths:\")\n",
    "    for idx, row in group.iterrows():\n",
    "        print(f\"  {row['file_path']} - {row['file_size_h']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
